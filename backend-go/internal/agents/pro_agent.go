package agents

import (
	"context"
	"fmt"
	"log"
	"net/url"
	"strings"
	"sync"
	"time"

	"github.com/Vova4o/bootCamp2025CaseSber/backend/internal/models"
	"github.com/Vova4o/bootCamp2025CaseSber/backend/internal/tools"
	"github.com/Vova4o/bootCamp2025CaseSber/backend/internal/utils"
)

type ProAgent struct {
	searchClient      *tools.SearchClient
	llmClient         *tools.LLMClient
	reranker          *tools.BM25Reranker
	credibilityScorer *tools.CredibilityScorer
	timeout           time.Duration
}

func NewProAgent(searchClient *tools.SearchClient, llmClient *tools.LLMClient) *ProAgent {
	return &ProAgent{
		searchClient:      searchClient,
		llmClient:         llmClient,
		reranker:          tools.NewBM25Reranker(),
		credibilityScorer: tools.NewCredibilityScorer(),
		timeout:           20 * time.Second, // Global timeout
	}
}

func (a *ProAgent) Process(ctx context.Context, query string) (*models.SearchResponse, error) {
	return a.ProcessWithContext(ctx, query, nil)
}

func (a *ProAgent) ProcessWithContext(
	ctx context.Context,
	query string,
	conversationHistory []models.Message,
) (*models.SearchResponse, error) {
	// Apply global timeout
	ctx, cancel := context.WithTimeout(ctx, a.timeout)
	defer cancel()

	queryLang := detectLanguage(query)
	log.Printf("Pro mode processing: %s (lang: %s, with context: %v)",
		query, queryLang, len(conversationHistory) > 0)

	reasoningSteps := []string{}
	searchQuery := query

	// Step 1: Enhance query with context
	if len(conversationHistory) > 0 {
		if queryLang == "ru" {
			reasoningSteps = append(reasoningSteps, "üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –¥–∏–∞–ª–æ–≥–∞...")
		} else {
			reasoningSteps = append(reasoningSteps, "üîç Analyzing previous conversation context...")
		}

		var contextPrompt strings.Builder
		if queryLang == "ru" {
			contextPrompt.WriteString("–ü—Ä–µ–¥—ã–¥—É—â–∞—è –±–µ—Å–µ–¥–∞:\n")
		} else {
			contextPrompt.WriteString("Previous conversation:\n")
		}

		start := len(conversationHistory) - 6
		if start < 0 {
			start = 0
		}
		for _, msg := range conversationHistory[start:] {
			role := msg.Role
			if queryLang == "ru" {
				if msg.Role == "user" {
					role = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å"
				} else {
					role = "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç"
				}
			}
			contextPrompt.WriteString(fmt.Sprintf("\n%s: %s\n", role, msg.Content))
		}

		var enhancePrompt string
		if queryLang == "ru" {
			enhancePrompt = fmt.Sprintf(`%s

–¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å: %s

–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π —Ç–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å —Ç–∞–∫, —á—Ç–æ–±—ã –æ–Ω –±—ã–ª —Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∏ –≤–∫–ª—é—á–∞–ª –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å:`, contextPrompt.String(), query)
		} else {
			enhancePrompt = fmt.Sprintf(`%s

Current question: %s

Rephrase the current question to be self-contained and include important information from context. Enhanced search query:`, contextPrompt.String(), query)
		}

		enhanced, err := a.llmClient.Complete(ctx, enhancePrompt, 0.3, 200)
		if err != nil {
			log.Printf("‚ö†Ô∏è  LLM failed to enhance query, using original: %v", err)
			if queryLang == "ru" {
				reasoningSteps = append(reasoningSteps, "‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É—é –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å (LLM –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω)")
			} else {
				reasoningSteps = append(reasoningSteps, "‚ö†Ô∏è Using original query (LLM unavailable)")
			}
		} else if enhanced != "" {
			searchQuery = strings.TrimSpace(enhanced)
			searchQuery = strings.Trim(searchQuery, `"'`)
			searchQuery = strings.TrimSpace(searchQuery)

			if searchQuery == "" {
				searchQuery = query
				log.Printf("‚ö†Ô∏è  Enhanced query was empty after cleanup")
			}

			if queryLang == "ru" {
				reasoningSteps = append(reasoningSteps, fmt.Sprintf("‚ú® –£–ª—É—á—à–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: \"%s\"", searchQuery))
			} else {
				reasoningSteps = append(reasoningSteps, fmt.Sprintf("‚ú® Enhanced query: \"%s\"", searchQuery))
			}
		} else {
			log.Printf("‚ö†Ô∏è  LLM returned empty enhanced query")
			if queryLang == "ru" {
				reasoningSteps = append(reasoningSteps, "‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑—É—é –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π –∑–∞–ø—Ä–æ—Å")
			} else {
				reasoningSteps = append(reasoningSteps, "‚ö†Ô∏è Using original query")
			}
		}
	} else {
		if queryLang == "ru" {
			reasoningSteps = append(reasoningSteps, "üìù –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –ø–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
		} else {
			reasoningSteps = append(reasoningSteps, "üìù Processing first query without context")
		}
	}

	// Step 2: Detect if multi-hop is needed
	needsMultiHop := a.detectMultiHop(query)

	var allResults []models.TavilyResult

	if needsMultiHop {
		if queryLang == "ru" {
			reasoningSteps = append(reasoningSteps, "üî¨ –û–±–Ω–∞—Ä—É–∂–µ–Ω —Å–ª–æ–∂–Ω—ã–π –≤–æ–ø—Ä–æ—Å - –ø—Ä–∏–º–µ–Ω—è—é multi-hop reasoning")
		} else {
			reasoningSteps = append(reasoningSteps, "üî¨ Complex question detected - applying multi-hop reasoning")
		}

		subQueries := a.generateSubQueries(ctx, searchQuery, queryLang)
		if queryLang == "ru" {
			reasoningSteps = append(reasoningSteps, fmt.Sprintf("üìã –†–∞–∑–±–∏–ª –Ω–∞ %d –ø–æ–¥–≤–æ–ø—Ä–æ—Å–∞", len(subQueries)))
		} else {
			reasoningSteps = append(reasoningSteps, fmt.Sprintf("üìã Split into %d sub-questions", len(subQueries)))
		}

		// Try parallel search
		allResults = a.parallelSubQuerySearch(ctx, subQueries, queryLang, &reasoningSteps)

		// FALLBACK: If insufficient results from multi-hop
		if len(allResults) < 3 {
			log.Printf("üîÑ Multi-hop insufficient results (%d), falling back to direct search", len(allResults))

			if queryLang == "ru" {
				reasoningSteps = append(reasoningSteps,
					fmt.Sprintf("üîÑ –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (%d), –≤—ã–ø–æ–ª–Ω—è—é –ø—Ä—è–º–æ–π –ø–æ–∏—Å–∫", len(allResults)))
			} else {
				reasoningSteps = append(reasoningSteps,
					fmt.Sprintf("üîÑ Insufficient results (%d), performing direct search", len(allResults)))
			}

			directResults, err := a.searchClient.Search(ctx, searchQuery, 15, true)
			if err != nil {
				log.Printf("‚ùå Fallback search also failed: %v", err)
				// Return what we have from multi-hop
			} else {
				// Merge results, prioritizing multi-hop
				allResults = append(allResults, directResults.Results...)
				log.Printf("‚úÖ Fallback search added %d results", len(directResults.Results))
			}
		}

		if queryLang == "ru" {
			reasoningSteps = append(reasoningSteps,
				fmt.Sprintf("üìö –°–æ–±—Ä–∞–Ω–æ %d –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", len(allResults)))
		} else {
			reasoningSteps = append(reasoningSteps,
				fmt.Sprintf("üìö Collected %d sources", len(allResults)))
		}
	} else {
		// Regular search
		log.Printf("üîé Executing search with query: %s", searchQuery)
		if queryLang == "ru" {
			reasoningSteps = append(reasoningSteps, fmt.Sprintf("üîé –ò—â—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –∑–∞–ø—Ä–æ—Å—É: \"%s\"", searchQuery))
		} else {
			reasoningSteps = append(reasoningSteps, fmt.Sprintf("üîé Searching for: \"%s\"", searchQuery))
		}

		searchResults, err := a.searchClient.Search(ctx, searchQuery, 15, true)
		if err != nil {
			log.Printf("‚ùå Search failed: %v", err)
			return nil, fmt.Errorf("search failed: %w", err)
		}

		allResults = searchResults.Results
		log.Printf("‚úÖ Search returned %d results", len(allResults))
		if queryLang == "ru" {
			reasoningSteps = append(reasoningSteps, fmt.Sprintf("‚úÖ –ù–∞–π–¥–µ–Ω–æ %d –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", len(allResults)))
		} else {
			reasoningSteps = append(reasoningSteps, fmt.Sprintf("‚úÖ Found %d sources", len(allResults)))
		}
	}

	if len(allResults) == 0 {
		var answer string
		if queryLang == "ru" {
			answer = "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É."
		} else {
			answer = "Could not find relevant information for your query."
		}

		return &models.SearchResponse{
			Query:     query,
			Mode:      "pro",
			Answer:    answer,
			Sources:   []models.Source{},
			Reasoning: strings.Join(reasoningSteps, "\n"),
		}, nil
	}

	// Step 3: Semantic Reranking —Å BM25
	if queryLang == "ru" {
		reasoningSteps = append(reasoningSteps, "üéØ –ü—Ä–∏–º–µ–Ω—è—é —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –ø–µ—Ä–µ–æ—Ü–µ–Ω–∫—É —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (BM25)")
	} else {
		reasoningSteps = append(reasoningSteps, "üéØ Applying semantic re-ranking (BM25)")
	}
	allResults = a.reranker.Rerank(searchQuery, allResults)

	// Step 4: Credibility Scoring
	if queryLang == "ru" {
		reasoningSteps = append(reasoningSteps, "‚≠ê –û—Ü–µ–Ω–∏–≤–∞—é –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
	} else {
		reasoningSteps = append(reasoningSteps, "‚≠ê Evaluating source credibility")
	}
	allResults = a.credibilityScorer.RankSources(allResults)

	// Step 5: Ensure Domain Diversity
	if queryLang == "ru" {
		reasoningSteps = append(reasoningSteps, "üåê –û–±–µ—Å–ø–µ—á–∏–≤–∞—é —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤")
	} else {
		reasoningSteps = append(reasoningSteps, "üåê Ensuring source diversity")
	}
	topResults := a.selectDiverseSources(allResults, 10)

	// Step 6: Cross-verification
	if queryLang == "ru" {
		reasoningSteps = append(reasoningSteps, "üîç –ü—Ä–æ–≤–µ—Ä—è—é –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –º–µ–∂–¥—É –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏")
	} else {
		reasoningSteps = append(reasoningSteps, "üîç Cross-verifying information across sources")
	}
	verification := a.crossVerify(topResults, queryLang)
	if verification != "" {
		reasoningSteps = append(reasoningSteps, verification)
	}

	// Step 7: Format sources for LLM (top 8 for context window)
	var sourcesContext strings.Builder
	displaySources := topResults
	if len(displaySources) > 8 {
		displaySources = displaySources[:8]
	}

	for i, result := range displaySources {
		content := result.Content
		if result.RawContent != "" {
			content = result.RawContent
		}
		
		// Sanitize and truncate safely
		content = utils.SanitizeUTF8(content)
		if len(content) > 800 {
			content = utils.TruncateUTF8WithEllipsis(content, 800)
		}

		if queryLang == "ru" {
			sourcesContext.WriteString(fmt.Sprintf(
				"–ò—Å—Ç–æ—á–Ω–∏–∫ %d [–î–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç—å: %.2f] (%s):\n%s\n\n",
				i+1, result.Credibility, result.Title, content,
			))
		} else {
			sourcesContext.WriteString(fmt.Sprintf(
				"Source %d [Credibility: %.2f] (%s):\n%s\n\n",
				i+1, result.Credibility, result.Title, content,
			))
		}
	}

	// Step 8: Build LLM prompt
	var promptBuilder strings.Builder
	if queryLang == "ru" {
		promptBuilder.WriteString(`–¢—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤ —Ä–µ–∂–∏–º–µ Pro —Å –≥–ª—É–±–æ–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞:
1. –î–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π, —Ö–æ—Ä–æ—à–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
2. –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å —É—á–µ—Ç–æ–º –∏—Ö –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏
3. –£–∫–∞–∑–∞—Ç—å, –µ—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤–∞ –∏–ª–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞
4. –î–µ–ª–∞—Ç—å –≤—ã–≤–æ–¥—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ–∫—Ä–µ—Å—Ç–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏

`)
	} else {
		promptBuilder.WriteString(`You are a Pro research assistant with deep analysis capabilities.

Your task:
1. Provide a detailed, well-reasoned answer
2. Use information from sources considering their credibility
3. Indicate if information is contradictory or insufficient
4. Draw conclusions based on cross-verification

`)
	}

	if len(conversationHistory) > 0 {
		if queryLang == "ru" {
			promptBuilder.WriteString("\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞:\n")
		} else {
			promptBuilder.WriteString("\nConversation context:\n")
		}
		start := len(conversationHistory) - 4
		if start < 0 {
			start = 0
		}
		for _, msg := range conversationHistory[start:] {
			promptBuilder.WriteString(fmt.Sprintf("%s: %s\n", msg.Role, msg.Content))
		}
		promptBuilder.WriteString("\n")
	}

	if queryLang == "ru" {
		promptBuilder.WriteString(fmt.Sprintf("–í–æ–ø—Ä–æ—Å: %s\n\n", query))
		promptBuilder.WriteString("–ù–∞–π–¥–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –ø–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏ –∏ –¥–æ—Å—Ç–æ–≤–µ—Ä–Ω–æ—Å—Ç–∏):\n")
		promptBuilder.WriteString(sourcesContext.String())
		promptBuilder.WriteString("\n–ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç —Å –∞–Ω–∞–ª–∏–∑–æ–º:")
	} else {
		promptBuilder.WriteString(fmt.Sprintf("Question: %s\n\n", query))
		promptBuilder.WriteString("Found information (sorted by relevance and credibility):\n")
		promptBuilder.WriteString(sourcesContext.String())
		promptBuilder.WriteString("\nDetailed answer with analysis:")
	}

	if queryLang == "ru" {
		reasoningSteps = append(reasoningSteps, "üí° –§–æ—Ä–º–∏—Ä—É—é —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç —Å —É—á—ë—Ç–æ–º –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...")
	} else {
		reasoningSteps = append(reasoningSteps, "üí° Generating final answer based on all data...")
	}

	// Step 9: Generate answer
	answer, err := a.llmClient.Complete(ctx, promptBuilder.String(), 0.7, 1200)
	if err != nil {
		return nil, fmt.Errorf("LLM completion failed: %w", err)
	}

	// Step 10: Format sources with UTF-8 safety
	sources := make([]models.Source, 0)
	for i, result := range displaySources {
		if i >= 8 {
			break
		}
		
		snippet := utils.SanitizeUTF8(result.Snippet)
		if len(snippet) > 200 {
			snippet = utils.TruncateUTF8WithEllipsis(snippet, 200)
		}
		
		sources = append(sources, models.Source{
			Title:       utils.SanitizeUTF8(result.Title),
			URL:         result.URL,
			Snippet:     snippet,
			Credibility: result.Credibility,
		})
	}

	return &models.SearchResponse{
		Query:       query,
		Mode:        "pro",
		Answer:      answer,
		Sources:     sources,
		Reasoning:   strings.Join(reasoningSteps, "\n"),
		ContextUsed: len(conversationHistory) > 0,
	}, nil
}

// parallelSubQuerySearch performs parallel searches for sub-queries
func (a *ProAgent) parallelSubQuerySearch(
	ctx context.Context,
	subQueries []string,
	queryLang string,
	reasoningSteps *[]string,
) []models.TavilyResult {
	type searchResult struct {
		results []models.TavilyResult
		query   string
		err     error
	}

	// Try parallel search with extended timeout
	resultsChan := make(chan searchResult, len(subQueries))
	var wg sync.WaitGroup

	for _, subQuery := range subQueries {
		wg.Add(1)
		go func(q string) {
			defer wg.Done()

			// Increased per-query timeout to handle slow responses
			queryCtx, cancel := context.WithTimeout(ctx, 12*time.Second)
			defer cancel()

			res, err := a.searchClient.Search(queryCtx, q, 5, true)
			if err != nil {
				log.Printf("Sub-query search failed for '%s': %v", q, err)
				resultsChan <- searchResult{nil, q, err}
				return
			}

			resultsChan <- searchResult{res.Results, q, nil}
		}(subQuery)
	}

	// Close channel when all goroutines finish
	go func() {
		wg.Wait()
		close(resultsChan)
	}()

	// Collect results
	allResults := make([]models.TavilyResult, 0)
	successCount := 0
	failCount := 0

	for sr := range resultsChan {
		if sr.err != nil {
			failCount++
			if queryLang == "ru" {
				*reasoningSteps = append(*reasoningSteps,
					fmt.Sprintf("  ‚ö†Ô∏è –ü–æ–¥–∑–∞–ø—Ä–æ—Å –ø—Ä–æ–ø—É—â–µ–Ω (timeout): %s",
						truncateQuery(sr.query, 60)))
			} else {
				*reasoningSteps = append(*reasoningSteps,
					fmt.Sprintf("  ‚ö†Ô∏è Sub-query skipped (timeout): %s",
						truncateQuery(sr.query, 60)))
			}
			continue
		}

		successCount++
		if queryLang == "ru" {
			*reasoningSteps = append(*reasoningSteps,
				fmt.Sprintf("  ‚úì %s (%d —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)",
					truncateQuery(sr.query, 60), len(sr.results)))
		} else {
			*reasoningSteps = append(*reasoningSteps,
				fmt.Sprintf("  ‚úì %s (%d results)",
					truncateQuery(sr.query, 60), len(sr.results)))
		}

		allResults = append(allResults, sr.results...)
	}

	// FALLBACK: If most sub-queries failed or not enough results
	if failCount >= len(subQueries)/2 || len(allResults) < 3 {
		log.Printf("‚ö†Ô∏è Multi-hop fallback: %d/%d sub-queries failed, switching to direct search",
			failCount, len(subQueries))

		if queryLang == "ru" {
			*reasoningSteps = append(*reasoningSteps,
				fmt.Sprintf("‚ö†Ô∏è –ü–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ –ø—Ä—è–º–æ–π –ø–æ–∏—Å–∫ (–ø–æ–¥–∑–∞–ø—Ä–æ—Å—ã: —É—Å–ø–µ—Ö %d, —Ñ–µ–π–ª %d)",
					successCount, failCount))
		} else {
			*reasoningSteps = append(*reasoningSteps,
				fmt.Sprintf("‚ö†Ô∏è Switching to direct search (sub-queries: success %d, failed %d)",
					successCount, failCount))
		}

		return allResults // Return partial results, caller will handle direct search
	}

	return allResults
}

// Helper function to truncate long queries
func truncateQuery(query string, maxLen int) string {
	return utils.TruncateUTF8(query, maxLen)
}

// selectDiverseSources ensures domain diversity in results
func (a *ProAgent) selectDiverseSources(results []models.TavilyResult, maxResults int) []models.TavilyResult {
	selected := make([]models.TavilyResult, 0, maxResults)
	domainCounts := make(map[string]int)
	maxPerDomain := 2 // Maximum 2 results from same domain

	for _, result := range results {
		if len(selected) >= maxResults {
			break
		}

		domain := extractDomain(result.URL)
		if domain == "" {
			continue
		}

		// Allow up to maxPerDomain results from same domain
		if domainCounts[domain] < maxPerDomain {
			selected = append(selected, result)
			domainCounts[domain]++
		}
	}

	// If we didn't get enough diverse results, fill remaining slots
	if len(selected) < maxResults {
		for _, result := range results {
			if len(selected) >= maxResults {
				break
			}

			domain := extractDomain(result.URL)
			if domainCounts[domain] >= maxPerDomain {
				// Allow one more from this domain
				found := false
				for _, s := range selected {
					if s.URL == result.URL {
						found = true
						break
					}
				}
				if !found {
					selected = append(selected, result)
				}
			}
		}
	}

	log.Printf("üìä Domain diversity: %d unique domains from %d sources",
		len(domainCounts), len(selected))

	return selected
}

// detectMultiHop determines if multi-hop reasoning is needed (improved)
func (a *ProAgent) detectMultiHop(query string) bool {
	queryLower := strings.ToLower(query)

	// Strong indicators for multi-hop
	strongIndicators := []string{
		// Comparison
		"—Å—Ä–∞–≤–Ω–∏", "compare", "–æ—Ç–ª–∏—á–∏—è", "difference", "—Ä–∞–∑–ª–∏—á–∏—è",
		"—Ä–∞–∑–Ω–∏—Ü–∞ –º–µ–∂–¥—É", "difference between",
		// Causation
		"–∫–∞–∫ —Å–≤—è–∑–∞–Ω—ã", "relationship", "–≤–∑–∞–∏–º–æ—Å–≤—è–∑—å",
		"–≤–ª–∏—è–Ω–∏–µ", "influence", "impact",
		"–ø—Ä–∏—á–∏–Ω—ã –∏ —Å–ª–µ–¥—Å—Ç–≤–∏—è", "causes and effects",
		"—á—Ç–æ –ø—Ä–∏–≤–µ–ª–æ –∫", "what led to", "how did",
		// Analysis
		"advantages and disadvantages", "pros and cons",
		"–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–∫–∏", "–∑–∞ –∏ –ø—Ä–æ—Ç–∏–≤",
	}

	for _, indicator := range strongIndicators {
		if strings.Contains(queryLower, indicator) {
			return true
		}
	}

	// Only for VERY long queries with multiple concepts
	words := strings.Fields(query)
	if len(words) > 20 {
		// Check if query has multiple question words/concepts
		questionWords := 0
		for _, word := range []string{"what", "how", "why", "–∫–æ–≥–¥–∞", "–∫–∞–∫", "–ø–æ—á–µ–º—É", "—á—Ç–æ"} {
			if strings.Contains(queryLower, word) {
				questionWords++
			}
		}
		return questionWords >= 2
	}

	return false
}

// generateSubQueries splits complex query into sub-questions
func (a *ProAgent) generateSubQueries(ctx context.Context, query string, lang string) []string {
	var prompt string
	if lang == "ru" {
		prompt = fmt.Sprintf(`–†–∞–∑–±–µ–π —Å–ª–æ–∂–Ω—ã–π –≤–æ–ø—Ä–æ—Å –Ω–∞ 2-3 –ø—Ä–æ—Å—Ç—ã—Ö –ø–æ–¥–≤–æ–ø—Ä–æ—Å–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.

–í–æ–ø—Ä–æ—Å: %s

–ü–æ–¥–≤–æ–ø—Ä–æ—Å—ã (–∫–∞–∂–¥—ã–π —Å –Ω–æ–≤–æ–π —Å—Ç—Ä–æ–∫–∏, –±–µ–∑ –Ω—É–º–µ—Ä–∞—Ü–∏–∏):`, query)
	} else {
		prompt = fmt.Sprintf(`Break down this complex question into 2-3 simple sub-questions for information search.

Question: %s

Sub-questions (one per line, no numbering):`, query)
	}

	response, err := a.llmClient.Complete(ctx, prompt, 0.3, 300)
	if err != nil {
		log.Printf("Failed to generate sub-queries: %v", err)
		return []string{query}
	}

	lines := strings.Split(response, "\n")
	subQueries := make([]string, 0)

	for _, line := range lines {
		line = strings.TrimSpace(line)
		// Remove various prefixes
		line = strings.TrimPrefix(line, "- ")
		line = strings.TrimPrefix(line, "‚Ä¢ ")
		line = strings.TrimPrefix(line, "* ")

		// Remove numbering
		for i := 1; i <= 9; i++ {
			line = strings.TrimPrefix(line, fmt.Sprintf("%d. ", i))
			line = strings.TrimPrefix(line, fmt.Sprintf("%d) ", i))
		}

		line = strings.TrimSpace(line)

		// Only add substantial queries
		if len(line) > 10 &&
			!strings.Contains(strings.ToLower(line), "sub-question") &&
			!strings.Contains(strings.ToLower(line), "–ø–æ–¥–≤–æ–ø—Ä–æ—Å") {
			subQueries = append(subQueries, line)
		}
	}

	if len(subQueries) == 0 {
		return []string{query}
	}

	// Limit to 3 sub-queries
	if len(subQueries) > 3 {
		subQueries = subQueries[:3]
	}

	return subQueries
}

// crossVerify checks consistency between sources
func (a *ProAgent) crossVerify(results []models.TavilyResult, lang string) string {
	if len(results) < 2 {
		return ""
	}

	commonPhrases := make(map[string]int)

	for _, result := range results {
		words := strings.Fields(strings.ToLower(result.Content))

		// Look for 3-4 word phrases
		for i := 0; i < len(words)-2; i++ {
			phrase := strings.Join(words[i:i+3], " ")
			if len(phrase) > 15 {
				commonPhrases[phrase]++
			}
		}
	}

	// Count facts verified by multiple sources
	verifiedCount := 0
	for _, count := range commonPhrases {
		if count >= 2 {
			verifiedCount++
		}
	}

	if lang == "ru" {
		if verifiedCount > 3 {
			return fmt.Sprintf("‚úì –ù–∞–π–¥–µ–Ω–æ %d+ —Ñ–∞–∫—Ç–æ–≤, –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–Ω—ã—Ö –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º–∏", verifiedCount)
		} else if verifiedCount > 0 {
			return "‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ñ–∞–∫—Ç—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω—ã —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∏–º –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º"
		}
		return "‚ö†Ô∏è –ò—Å—Ç–æ—á–Ω–∏–∫–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç —Ä–∞–∑–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é - —Ç—Ä–µ–±—É–µ—Ç—Å—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞"
	} else {
		if verifiedCount > 3 {
			return fmt.Sprintf("‚úì Found %d+ facts verified by multiple sources", verifiedCount)
		} else if verifiedCount > 0 {
			return "‚ö†Ô∏è Some facts verified by only one source"
		}
		return "‚ö†Ô∏è Sources contain different information - additional verification needed"
	}
}

// detectLanguage determines text language
func detectLanguage(text string) string {
	cyrillicCount := 0
	totalLetters := 0

	for _, r := range text {
		if (r >= '–∞' && r <= '—è') || (r >= '–ê' && r <= '–Ø') || r == '—ë' || r == '–Å' {
			cyrillicCount++
			totalLetters++
		} else if (r >= 'a' && r <= 'z') || (r >= 'A' && r <= 'Z') {
			totalLetters++
		}
	}

	if totalLetters == 0 {
		return "en"
	}

	if float64(cyrillicCount)/float64(totalLetters) > 0.3 {
		return "ru"
	}

	return "en"
}

// extractDomain extracts clean domain from URL
func extractDomain(urlStr string) string {
	parsedURL, err := url.Parse(urlStr)
	if err != nil {
		return ""
	}

	hostname := strings.ToLower(parsedURL.Hostname())

	// Remove www. prefix
	hostname = strings.TrimPrefix(hostname, "www.")

	return hostname
}