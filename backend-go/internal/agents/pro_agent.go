package agents

import (
	"context"
	"fmt"
	"log"
	"strings"

	"github.com/Vova4o/bootCamp2025CaseSber/backend/internal/models"
	"github.com/Vova4o/bootCamp2025CaseSber/backend/internal/tools"
)

type ProAgent struct {
	searchClient *tools.SearchClient
	llmClient    *tools.LLMClient
}

func NewProAgent(searchClient *tools.SearchClient, llmClient *tools.LLMClient) *ProAgent {
	return &ProAgent{
		searchClient: searchClient,
		llmClient:    llmClient,
	}
}

func (a *ProAgent) Process(ctx context.Context, query string) (*models.SearchResponse, error) {
	return a.ProcessWithContext(ctx, query, nil)
}

func (a *ProAgent) ProcessWithContext(
	ctx context.Context,
	query string,
	conversationHistory []models.Message,
) (*models.SearchResponse, error) {
	log.Printf("Pro mode processing: %s (with context: %v)", query, len(conversationHistory) > 0)

	reasoningSteps := []string{}
	searchQuery := query

	// Step 1: Enhance query with context if available
	if len(conversationHistory) > 0 {
		reasoningSteps = append(reasoningSteps, "üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É—é –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –¥–∏–∞–ª–æ–≥–∞...")

		var contextPrompt strings.Builder
		contextPrompt.WriteString("–ü—Ä–µ–¥—ã–¥—É—â–∞—è –±–µ—Å–µ–¥–∞:\n")
		// Take last 6 messages (3 pairs)
		start := len(conversationHistory) - 6
		if start < 0 {
			start = 0
		}
		for _, msg := range conversationHistory[start:] {
			role := "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å"
			if msg.Role == "assistant" {
				role = "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç"
			}
			contextPrompt.WriteString(fmt.Sprintf("\n%s: %s\n", role, msg.Content))
		}

		enhancePrompt := fmt.Sprintf(`%s

–¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å: %s

–ü–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π —Ç–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å —Ç–∞–∫, —á—Ç–æ–±—ã –æ–Ω –±—ã–ª —Å–∞–º–æ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∏ –≤–∫–ª—é—á–∞–ª –≤–∞–∂–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å:`, contextPrompt.String(), query)

		enhanced, err := a.llmClient.Complete(ctx, enhancePrompt, 0.3, 200)
		if err == nil && enhanced != "" {
			searchQuery = enhanced
			reasoningSteps = append(reasoningSteps, fmt.Sprintf("‚ú® –£–ª—É—á—à–µ–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: %s", searchQuery))
		}
	} else {
		reasoningSteps = append(reasoningSteps, "üìù –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é –ø–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å –±–µ–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞")
	}

	// Step 2: Search for information
	reasoningSteps = append(reasoningSteps, fmt.Sprintf("üîé –ò—â—É –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –∑–∞–ø—Ä–æ—Å—É: %s", searchQuery))

	searchResults, err := a.searchClient.Search(ctx, searchQuery, 10, true)
	if err != nil {
		return nil, fmt.Errorf("search failed: %w", err)
	}

	if len(searchResults.Results) == 0 {
		return &models.SearchResponse{
			Query:     query,
			Mode:      "pro",
			Answer:    "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É.",
			Sources:   []models.Source{},
			Reasoning: strings.Join(reasoningSteps, "\n"),
		}, nil
	}

	reasoningSteps = append(reasoningSteps, fmt.Sprintf("‚úÖ –ù–∞–π–¥–µ–Ω–æ %d –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤", len(searchResults.Results)))

	// Step 3: Format sources for LLM
	var sourcesContext strings.Builder
	for i, result := range searchResults.Results {
		if i >= 5 {
			break
		}
		content := result.Content
		if result.RawContent != "" {
			content = result.RawContent
		}
		if len(content) > 1000 {
			content = content[:1000]
		}
		sourcesContext.WriteString(fmt.Sprintf("–ò—Å—Ç–æ—á–Ω–∏–∫ %d (%s):\n%s\n\n", i+1, result.Title, content))
	}

	// Step 4: Build LLM prompt with context
	var promptBuilder strings.Builder
	promptBuilder.WriteString(`–¢—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤ —Ä–µ–∂–∏–º–µ Pro.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –¥–∞—Ç—å –ø–æ–¥—Ä–æ–±–Ω—ã–π, —Ö–æ—Ä–æ—à–æ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç —Å —É—á—ë—Ç–æ–º:
1. –ö–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø—Ä–µ–¥—ã–¥—É—â–µ–π –±–µ—Å–µ–¥—ã (–µ—Å–ª–∏ –µ—Å—Ç—å)
2. –ù–∞–π–¥–µ–Ω–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
3. –ü—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–∫—Ç–æ–≤

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞:
- –ü—Ä—è–º–æ–π –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å
- –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Ñ–∞–∫—Ç–∞–º–∏ –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
- –ï—Å–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –ø—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏–≤–∞ - —É–∫–∞–∂–∏ —ç—Ç–æ

`)

	// Add conversation history
	if len(conversationHistory) > 0 {
		promptBuilder.WriteString("\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –¥–∏–∞–ª–æ–≥–∞:\n")
		start := len(conversationHistory) - 4
		if start < 0 {
			start = 0
		}
		for _, msg := range conversationHistory[start:] {
			promptBuilder.WriteString(fmt.Sprintf("%s: %s\n", msg.Role, msg.Content))
		}
		promptBuilder.WriteString("\n")
	}

	promptBuilder.WriteString(fmt.Sprintf("–í–æ–ø—Ä–æ—Å: %s\n\n", query))
	promptBuilder.WriteString("–ù–∞–π–¥–µ–Ω–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è:\n")
	promptBuilder.WriteString(sourcesContext.String())
	promptBuilder.WriteString("\n–û—Ç–≤–µ—Ç:")

	reasoningSteps = append(reasoningSteps, "üí° –§–æ—Ä–º–∏—Ä—É—é –æ—Ç–≤–µ—Ç —Å —É—á—ë—Ç–æ–º –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö...")

	// Step 5: Generate answer
	answer, err := a.llmClient.Complete(ctx, promptBuilder.String(), 0.7, 1000)
	if err != nil {
		return nil, fmt.Errorf("LLM completion failed: %w", err)
	}

	// Step 6: Format sources
	sources := make([]models.Source, 0)
	for i, result := range searchResults.Results {
		if i >= 5 {
			break
		}
		snippet := result.Snippet
		if len(snippet) > 200 {
			snippet = snippet[:200] + "..."
		}
		sources = append(sources, models.Source{
			Title:       result.Title,
			URL:         result.URL,
			Snippet:     snippet,
			Credibility: 0.85, // TODO: Implement real credibility scoring
		})
	}

	return &models.SearchResponse{
		Query:       query,
		Mode:        "pro",
		Answer:      answer,
		Sources:     sources,
		Reasoning:   strings.Join(reasoningSteps, "\n"),
		ContextUsed: len(conversationHistory) > 0,
	}, nil
}
